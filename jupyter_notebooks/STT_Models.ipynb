{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxG6FK8utBBB"
      },
      "source": [
        "# Speech-to-Text (STT) Notebook\n",
        "A structured workflow for development and experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGHCKQEhtIEy"
      },
      "source": [
        "## Environment Setup (pip installs, conda setup, dependencies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUFVXSTOtPAj"
      },
      "outputs": [],
      "source": [
        "# !pip install https://github.com/huggingface/transformers/archive/main.zip torchaudio peft soundfile\n",
        "# !pip install https://files.pythonhosted.org/packages/fc/ca/83398cfcd557360a3d7b2d732aee1c5f6999f68618d1645f38d53e14c9ff/vosk-0.3.45-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n",
        "# !pip -q install vosk pydub psutil jiwer\n",
        "# !pip -q install faster-whisper soundfile\n",
        "# !pip -q install transformers soundfile librosa jiwer psutil\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrquLnDrtI-9"
      },
      "source": [
        "## Imports (all Python imports in one place)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torchaudio OK\n"
          ]
        }
      ],
      "source": [
        "# pre-check for torchaudio\n",
        "import os, ctypes\n",
        "# Ensure libgomp is in the global symbol table before torchaudio loads\n",
        "try:\n",
        "    ctypes.CDLL(\"libgomp.so.1\", mode=os.RTLD_GLOBAL)\n",
        "except OSError:\n",
        "    # Fallback to absolute path if needed (adjust if your path differs)\n",
        "    ctypes.CDLL(\"/lib/aarch64-linux-gnu/libgomp.so.1\", mode=os.RTLD_GLOBAL)\n",
        "\n",
        "import torchaudio  # must come AFTER the preload\n",
        "print(\"torchaudio OK\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_dZQVLxds2UZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration, AutoProcessor, AutoModelForSpeechSeq2Seq\n",
        "from datasets import load_dataset\n",
        "\n",
        "import vosk\n",
        "\n",
        "from huggingface_hub import hf_hub_download"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xsLoeDztUDL"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsGkYVXOtVpo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFR1RaJDtYBL"
      },
      "source": [
        "## Data / Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7fzDASVtYds"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emasNkqOtY1H"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phRF74KItb1u"
      },
      "source": [
        "## Facebook s2t-small-librispeech-asr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj1nk8f55pMs"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hg0AMlvKtfqW",
        "outputId": "6cf512c2-e014-4ae4-8ee5-1d1971758c47"
      },
      "outputs": [],
      "source": [
        "import transformers, torch\n",
        "print(\"Transformers:\", transformers.__version__, \"| Torch CUDA:\", torch.cuda.is_available())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ntzbBQm5uV5",
        "outputId": "21c651e6-a83d-437a-c18b-3f3a12602d65"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n",
        "import sentencepiece\n",
        "\n",
        "MODEL_ID = \"facebook/s2t-small-librispeech-asr\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = Speech2TextProcessor.from_pretrained(MODEL_ID)\n",
        "model = Speech2TextForConditionalGeneration.from_pretrained(MODEL_ID)\n",
        "model.to(DEVICE)  # keep float32; this model is stable in fp32\n",
        "\n",
        "print(\"Loaded:\", MODEL_ID, \"| Device:\", DEVICE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_geA4mwe5w4B"
      },
      "source": [
        "### Compute Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1BfnqhW50f0",
        "outputId": "28fdad3d-f2ea-44f4-c022-c470f1d74ce5"
      },
      "outputs": [],
      "source": [
        "import time, psutil, numpy as np, soundfile as sf, librosa, torch\n",
        "\n",
        "AUDIO_PATH = \"ref.wav\"\n",
        "\n",
        "# Load audio as mono 16 kHz float32\n",
        "# (librosa handles resampling; S2T uses 16k log-mels)\n",
        "wave, sr = librosa.load(AUDIO_PATH, sr=16_000, mono=True)\n",
        "audio_dur_sec = len(wave) / 16_000.0\n",
        "\n",
        "# Features\n",
        "inputs = processor(wave, sampling_rate=16_000, return_tensors=\"pt\")\n",
        "input_features = inputs.input_features.to(DEVICE)\n",
        "attn_mask = inputs.attention_mask.to(DEVICE) if \"attention_mask\" in inputs else None\n",
        "\n",
        "# Decode\n",
        "t0 = time.time()\n",
        "with torch.inference_mode():\n",
        "    generated_ids = model.generate(\n",
        "        input_features,\n",
        "        attention_mask=attn_mask,\n",
        "        max_length=448,     # safe cap; adjust if your files are long\n",
        "        num_beams=1,        # greedy for speed\n",
        "        do_sample=False\n",
        "    )\n",
        "wall = time.time() - t0\n",
        "\n",
        "# Text\n",
        "s2t_transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "# Metrics\n",
        "rtf = wall / max(audio_dur_sec, 1e-6)\n",
        "proc = psutil.Process()\n",
        "mem_mb = proc.memory_info().rss / (1024**2)\n",
        "cpu_pct = psutil.cpu_percent(interval=0.2)\n",
        "\n",
        "print(\"— TRANSCRIPT —\")\n",
        "print(s2t_transcript if s2t_transcript else \"(empty)\")\n",
        "print(\"\\n— STATS —\")\n",
        "print(f\"duration_sec: {audio_dur_sec:.4f}\")\n",
        "print(f\"wall_time_sec: {wall:.4f}\")\n",
        "print(f\"real_time_factor: {rtf:.4f}\")\n",
        "print(f\"cpu_percent: {cpu_pct:.2f}\")\n",
        "print(f\"mem_rss_mb: {mem_mb:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pre-check\n",
        "import os, ctypes, sys\n",
        "\n",
        "# Try system libgomp first (adjust path if ldconfig shows a different one)\n",
        "CANDIDATES = [\n",
        "    \"libgomp.so.1\",\n",
        "    \"/lib/aarch64-linux-gnu/libgomp.so.1\",\n",
        "    \"/usr/lib/aarch64-linux-gnu/libgomp.so.1\",\n",
        "]\n",
        "\n",
        "loaded = False\n",
        "for p in CANDIDATES:\n",
        "    try:\n",
        "        ctypes.CDLL(p, mode=os.RTLD_GLOBAL)\n",
        "        loaded = True\n",
        "        break\n",
        "    except OSError:\n",
        "        pass\n",
        "\n",
        "if not loaded:\n",
        "    raise RuntimeError(\"Could not preload libgomp; check your system path.\")\n",
        "\n",
        "# OPTIONAL but helpful on small systems\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "os.environ.setdefault(\"OPENBLAS_NUM_THREADS\", \"1\")\n",
        "\n",
        "# Now it's safe to import sklearn/transformers/etc.\n",
        "import sklearn\n",
        "print(\"sklearn OK:\", sklearn.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0-zDTVEKdBr",
        "outputId": "fdb4e8ba-189c-47f2-d323-aa36e28018ea"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration, pipeline\n",
        "\n",
        "AUDIO_PATH = \"ref.wav\"\n",
        "MODEL_ID   = \"facebook/s2t-small-librispeech-asr\"\n",
        "\n",
        "processor = Speech2TextProcessor.from_pretrained(MODEL_ID)\n",
        "model = Speech2TextForConditionalGeneration.from_pretrained(MODEL_ID)\n",
        "asr_s2t = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        ")\n",
        "\n",
        "s2t_out = asr_s2t(\n",
        "    AUDIO_PATH,\n",
        "    chunk_length_s=15, stride_length_s=(2,2),\n",
        "    generate_kwargs=dict(\n",
        "        num_beams=3,\n",
        "        no_repeat_ngram_size=3,\n",
        "        repetition_penalty=1.2,\n",
        "        length_penalty=1.0,\n",
        "        max_new_tokens=200,\n",
        "        early_stopping=True,\n",
        "    ),\n",
        ")\n",
        "s2t_transcript = s2t_out[\"text\"]\n",
        "print(\"S2T:\", s2t_transcript[:200], \"…\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIUvj7ovx3Z7"
      },
      "source": [
        "## Vosk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLT6dVWb2GZP"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvLtd4NAx5_C",
        "outputId": "bff2ccc6-51a9-46d0-ae22-878b07c0e6a6"
      },
      "outputs": [],
      "source": [
        "import os, zipfile, urllib.request\n",
        "\n",
        "model_url = \"https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\"\n",
        "zip_path  = \"content/vosk-model-small-en-us-0.15.zip\"\n",
        "model_dir = \"content/vosk-model-small-en-us-0.15\"\n",
        "\n",
        "if not os.path.exists(model_dir):\n",
        "    print(\"Downloading Vosk small EN model...\")\n",
        "    urllib.request.urlretrieve(model_url, zip_path)\n",
        "    print(\"Unzipping...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zf:\n",
        "        zf.extractall(\"content\") # dictory path of the content path folder\n",
        "    os.remove(zip_path)\n",
        "else:\n",
        "    print(\"Model already present\")\n",
        "\n",
        "print(\"Model ready:\", model_dir, \"| Exists:\", os.path.isdir(model_dir))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvdiD7RR2Ubk"
      },
      "source": [
        "### Computer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quHUZPGJ2UCr",
        "outputId": "75accca8-aac4-416a-d665-64f6d95662c6"
      },
      "outputs": [],
      "source": [
        "import json, time, subprocess, tempfile, psutil\n",
        "from pathlib import Path\n",
        "from pydub import AudioSegment\n",
        "from vosk import Model, KaldiRecognizer\n",
        "\n",
        "AUDIO_PATH = Path(\"content/ref.wav\")\n",
        "MODEL_DIR  = Path(\"content/vosk-model-small-en-us-0.15\")\n",
        "\n",
        "def ensure_pcm16_mono_16k(in_path: Path) -> Path:\n",
        "    out_wav = Path(tempfile.gettempdir()) / f\"conv_{in_path.stem}_16k_mono.wav\"\n",
        "    cmd = [\n",
        "        \"ffmpeg\",\"-y\",\n",
        "        \"-i\", str(in_path),\n",
        "        \"-ac\",\"1\",\"-ar\",\"16000\",\"-f\",\"wav\",\"-acodec\",\"pcm_s16le\",\n",
        "        str(out_wav)\n",
        "    ]\n",
        "    # Colab has ffmpeg preinstalled\n",
        "    subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
        "    return out_wav\n",
        "\n",
        "def transcribe_file_vosk(model_dir: Path, audio_path: Path):\n",
        "    t0 = time.time()\n",
        "    model = Model(str(model_dir))\n",
        "    rec = KaldiRecognizer(model, 16000)\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    wav_path = ensure_pcm16_mono_16k(audio_path)\n",
        "    pcm = AudioSegment.from_wav(wav_path)\n",
        "    raw = pcm.raw_data\n",
        "\n",
        "    # ~30 ms chunking\n",
        "    chunk_ms = 30\n",
        "    step = int(16000 * 2 * chunk_ms / 1000)  # bytes per 30 ms (16k * 2 bytes)\n",
        "    for i in range(0, len(raw), step):\n",
        "        rec.AcceptWaveform(raw[i:i+step])\n",
        "\n",
        "    final_json = json.loads(rec.FinalResult())\n",
        "    text = final_json.get(\"text\", \"\")\n",
        "\n",
        "    wall = time.time() - t0\n",
        "    dur  = len(pcm) / 1000.0\n",
        "    rtf  = wall / max(dur, 1e-6)\n",
        "\n",
        "    proc = psutil.Process()\n",
        "    mem_mb = proc.memory_info().rss / (1024**2)\n",
        "    cpu_pct = psutil.cpu_percent(interval=0.2)\n",
        "\n",
        "    stats = dict(duration_sec=dur, wall_time_sec=wall, real_time_factor=rtf,\n",
        "                 cpu_percent=cpu_pct, mem_rss_mb=mem_mb)\n",
        "    return text, stats\n",
        "\n",
        "vosk_text, stats = transcribe_file_vosk(MODEL_DIR, AUDIO_PATH)\n",
        "\n",
        "print(\"— TRANSCRIPT —\")\n",
        "print(vosk_text if vosk_text else \"(empty)\")\n",
        "print(\"\\n— STATS —\")\n",
        "for k, v in stats.items():\n",
        "    print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-awhImc-vvZ3"
      },
      "source": [
        "## Facebook - Wav2Vec2-Base-960h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uffzb7Yq7SnQ"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reYsGO72v3Wk",
        "outputId": "d6a93aee-0e49-4368-c2d3-cb3f3bb2fca3"
      },
      "outputs": [],
      "source": [
        "import torch, psutil\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, pipeline\n",
        "\n",
        "MODEL_ID = \"facebook/wav2vec2-base-960h\"\n",
        "AUDIO_PATH = \"/content/ref.wav\"\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device_index = 0 if device == \"cuda\" else -1\n",
        "\n",
        "# Load processor + model\n",
        "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
        "\n",
        "# Optional: dynamic quantization on CPU to reduce RAM (safe to skip on GPU)\n",
        "if device == \"cpu\":\n",
        "    model = torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "\n",
        "model.to(device).eval()\n",
        "\n",
        "# Build a pipeline so we get robust long-audio chunking\n",
        "asr = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    device=device_index,\n",
        ")\n",
        "print(f\"Ready on {device} (quantized={device=='cpu'})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqJNyglt_9Ct"
      },
      "source": [
        "### Model Compute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNiRMt9O__ta",
        "outputId": "3096095c-3230-42fb-bbbb-09d7a5728fb8"
      },
      "outputs": [],
      "source": [
        "import time, soundfile as sf, psutil\n",
        "\n",
        "# Audio duration for RTF\n",
        "info = sf.info(AUDIO_PATH)\n",
        "audio_dur_sec = info.frames / max(info.samplerate, 1)\n",
        "\n",
        "t0 = time.time()\n",
        "out = asr(\n",
        "    AUDIO_PATH,\n",
        "    chunk_length_s=20,        # chunking keeps RAM flat\n",
        "    stride_length_s=(2, 2),   # small overlap\n",
        "    return_timestamps=\"word\", # <-- FIX for CTC pipelines\n",
        "    # ignore_warning=True,    # optional\n",
        ")\n",
        "wall = time.time() - t0\n",
        "rtf = wall / max(audio_dur_sec, 1e-6)\n",
        "\n",
        "proc = psutil.Process()\n",
        "mem_mb = proc.memory_info().rss / (1024**2)\n",
        "cpu_pct = psutil.cpu_percent(interval=0.2)\n",
        "\n",
        "wav2vec2_text = out[\"text\"].strip() if isinstance(out, dict) else str(out).strip()\n",
        "\n",
        "print(\"— TRANSCRIPT —\")\n",
        "print(wav2vec2_text if wav2vec2_text else \"(empty)\")\n",
        "\n",
        "print(\"\\n— STATS —\")\n",
        "print(f\"duration_sec: {audio_dur_sec:.4f}\")\n",
        "print(f\"wall_time_sec: {wall:.4f}\")\n",
        "print(f\"real_time_factor: {rtf:.4f}\")\n",
        "print(f\"cpu_percent: {cpu_pct:.2f}\")\n",
        "print(f\"mem_rss_mb: {mem_mb:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfNVQ3PMx9la"
      },
      "source": [
        "## OpenAI - Whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIqZotRA3_t4"
      },
      "source": [
        "### Model Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqAVAjfk4Bsg",
        "outputId": "bf91b4a3-4f28-43dc-8f24-94893841940f"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "/home/nvidia/Documents/STT-Model-Transformer/venv/lib/python3.8/site-packages/ctranslate2/../ctranslate2.libs/libgomp-d22c30c5.so.1.0.0: cannot allocate memory in static TLS block",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpsutil\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msoundfile\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WhisperModel\n\u001b[1;32m      4\u001b[0m AUDIO_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent/ref.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Choose size: \"tiny\", \"base\", (\"small\", \"medium\" also exist but heavier)\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/STT-Model-Transformer/venv/lib/python3.8/site-packages/faster_whisper/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decode_audio\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtranscribe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchedInferencePipeline, WhisperModel\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m available_models, download_model, format_timestamp\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfaster_whisper\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
            "File \u001b[0;32m~/Documents/STT-Model-Transformer/venv/lib/python3.8/site-packages/faster_whisper/transcribe.py:13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BinaryIO, Iterable, List, Optional, Tuple, Union\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mctranslate2\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtokenizers\u001b[39;00m\n",
            "File \u001b[0;32m~/Documents/STT-Model-Transformer/venv/lib/python3.8/site-packages/ctranslate2/__init__.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m         ctypes\u001b[38;5;241m.\u001b[39mCDLL(library)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mctranslate2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     22\u001b[0m         AsyncGenerationResult,\n\u001b[1;32m     23\u001b[0m         AsyncScoringResult,\n\u001b[1;32m     24\u001b[0m         AsyncTranslationResult,\n\u001b[1;32m     25\u001b[0m         DataType,\n\u001b[1;32m     26\u001b[0m         Device,\n\u001b[1;32m     27\u001b[0m         Encoder,\n\u001b[1;32m     28\u001b[0m         EncoderForwardOutput,\n\u001b[1;32m     29\u001b[0m         ExecutionStats,\n\u001b[1;32m     30\u001b[0m         GenerationResult,\n\u001b[1;32m     31\u001b[0m         GenerationStepResult,\n\u001b[1;32m     32\u001b[0m         Generator,\n\u001b[1;32m     33\u001b[0m         MpiInfo,\n\u001b[1;32m     34\u001b[0m         ScoringResult,\n\u001b[1;32m     35\u001b[0m         StorageView,\n\u001b[1;32m     36\u001b[0m         TranslationResult,\n\u001b[1;32m     37\u001b[0m         Translator,\n\u001b[1;32m     38\u001b[0m         contains_model,\n\u001b[1;32m     39\u001b[0m         get_cuda_device_count,\n\u001b[1;32m     40\u001b[0m         get_supported_compute_types,\n\u001b[1;32m     41\u001b[0m         set_random_seed,\n\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mctranslate2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_extensions\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mctranslate2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_log_level, set_log_level\n",
            "\u001b[0;31mImportError\u001b[0m: /home/nvidia/Documents/STT-Model-Transformer/venv/lib/python3.8/site-packages/ctranslate2/../ctranslate2.libs/libgomp-d22c30c5.so.1.0.0: cannot allocate memory in static TLS block"
          ]
        }
      ],
      "source": [
        "import torch, psutil, time, soundfile as sf\n",
        "from faster_whisper import WhisperModel\n",
        "\n",
        "AUDIO_PATH = \"content/ref.wav\"\n",
        "\n",
        "# Choose size: \"tiny\", \"base\", (\"small\", \"medium\" also exist but heavier)\n",
        "MODEL_SIZE = \"base\"\n",
        "\n",
        "# Auto-select device/precision\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "    COMPUTE_TYPE = \"float16\"   # good default for GPU\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "    COMPUTE_TYPE = \"int8\"      # efficient on CPU\n",
        "\n",
        "print(f\"Using model={MODEL_SIZE}, device={DEVICE}, compute_type={COMPUTE_TYPE}\")\n",
        "model = WhisperModel(MODEL_SIZE, device=DEVICE, compute_type=COMPUTE_TYPE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYGbu4eD4BQa"
      },
      "source": [
        "### Compute Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJvC57qv4H2T",
        "outputId": "1fac5f73-0904-427a-8b61-3ab9259b730c"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "# Load once to get duration for RTF\n",
        "info = sf.info(AUDIO_PATH)\n",
        "audio_dur_sec = info.frames / max(info.samplerate, 1)\n",
        "\n",
        "t0 = time.time()\n",
        "segments, info_rt = model.transcribe(\n",
        "    AUDIO_PATH,\n",
        "    language=\"en\",           # English only (skips auto-detect)\n",
        "    task=\"transcribe\",       # not \"translate\"\n",
        "    beam_size=1,             # greedy for speed; set >1 for accuracy\n",
        "    vad_filter=True,         # basic VAD can help on noisy files\n",
        ")\n",
        "# Collect text (order preserved)\n",
        "whisper_text = \" \".join(s.text.strip() for s in segments)\n",
        "\n",
        "wall = time.time() - t0\n",
        "rtf = wall / max(audio_dur_sec, 1e-6)\n",
        "\n",
        "proc = psutil.Process()\n",
        "mem_mb = proc.memory_info().rss / (1024**2)\n",
        "cpu_pct = psutil.cpu_percent(interval=0.2)\n",
        "\n",
        "print(\"— TRANSCRIPT —\")\n",
        "print(whisper_text if whisper_text else \"(empty)\")\n",
        "print(\"\\n— STATS —\")\n",
        "print(f\"duration_sec: {audio_dur_sec:.4f}\")\n",
        "print(f\"wall_time_sec: {wall:.4f}\")\n",
        "print(f\"real_time_factor: {rtf:.4f}\")\n",
        "print(f\"cpu_percent: {cpu_pct:.2f}\")\n",
        "print(f\"mem_rss_mb: {mem_mb:.2f}\")\n",
        "\n",
        "# Extra info from faster-whisper (when available)\n",
        "if info_rt is not None:\n",
        "    # info_rt might have attributes like duration, transcription options, etc.\n",
        "    try:\n",
        "        print(\"\\n— RUNTIME INFO —\")\n",
        "        print(f\"language: {info_rt.language}\")\n",
        "        print(f\"language_probability: {info_rt.language_probability:.4f}\")\n",
        "        print(f\"duration (reported): {info_rt.duration:.4f}\")\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0aq5ZovIjiQ"
      },
      "source": [
        "## Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYuNJgdtI3fz"
      },
      "outputs": [],
      "source": [
        "ref_str = \"It looks like your descending, sir I need to make sure you are climbing not descending, Gold is climbing 22G, say altitude 2500, 22G, 22G low altitude alert, climb the airplane maintain 5000, climb the airplane please, just level off the plane and climb the airplane up to 5000, when you can, sir, you appear to be descending again sir, Are you... say altitude. Tower the aircraft has crashed uhh... about half a mile in front of us into the houses\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "WbHIWD9aJnxF",
        "outputId": "14c98ecc-460c-4863-e2c9-b52cfeb5d336"
      },
      "outputs": [],
      "source": [
        "# %% ASR 4-way comparison with Accuracy + Correctness/Precision/F1\n",
        "# Expects these variables to be defined earlier:\n",
        "#   ref_str, vosk_text, whisper_text, wav2vec2_text, s2t_transcript\n",
        "# Optional installs (uncomment if needed):\n",
        "# !pip -q install pandas matplotlib\n",
        "\n",
        "import re, os, csv\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Normalization tuned for ATC-ish speech ----------\n",
        "def normalize_atc(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    # map common domain tokens\n",
        "    s = s.replace(\"22g\", \"two two golf\")\n",
        "    s = s.replace(\"5,000\", \"five thousand\").replace(\"5000\", \"five thousand\")\n",
        "    s = s.replace(\"2500\", \"two thousand five hundred\")\n",
        "    s = s.replace(\"gold\", \"golf\")\n",
        "    # drop fillers, punctuation, collapse spaces\n",
        "    s = re.sub(r\"\\b(uh+|um+)\\b\", \" \", s)\n",
        "    s = re.sub(r\"[‐–—…]\", \" \", s)          # normalize dashes/ellipses\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)         # strip punctuation\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def normalize_raw(s: str) -> str:\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# ---------- Levenshtein alignment to get S/D/I + matches ----------\n",
        "def align_counts(ref_tokens, hyp_tokens):\n",
        "    R, H = ref_tokens, hyp_tokens\n",
        "    n, m = len(R), len(H)\n",
        "    dp = [[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(1, n+1): dp[i][0] = i\n",
        "    for j in range(1, m+1): dp[0][j] = j\n",
        "    for i in range(1, n+1):\n",
        "        for j in range(1, m+1):\n",
        "            if R[i-1] == H[j-1]:\n",
        "                dp[i][j] = dp[i-1][j-1]\n",
        "            else:\n",
        "                dp[i][j] = min(dp[i-1][j] + 1,    # deletion\n",
        "                               dp[i][j-1] + 1,    # insertion\n",
        "                               dp[i-1][j-1] + 1)  # substitution\n",
        "    # backtrack to count\n",
        "    i, j = n, m\n",
        "    S = D = I = C = 0\n",
        "    while i > 0 or j > 0:\n",
        "        if i>0 and j>0 and R[i-1] == H[j-1] and dp[i][j] == dp[i-1][j-1]:\n",
        "            C += 1; i -= 1; j -= 1\n",
        "        elif i>0 and j>0 and dp[i][j] == dp[i-1][j-1] + 1:\n",
        "            S += 1; i -= 1; j -= 1\n",
        "        elif i>0 and dp[i][j] == dp[i-1][j] + 1:\n",
        "            D += 1; i -= 1\n",
        "        else:\n",
        "            I += 1; j -= 1\n",
        "    return {\"S\": S, \"D\": D, \"I\": I, \"C\": C, \"N\": n, \"M\": m}\n",
        "\n",
        "def metrics_from_tokens(ref_toks, hyp_toks):\n",
        "    c = align_counts(ref_toks, hyp_toks)\n",
        "    N, M = max(1, c[\"N\"]), max(1, c[\"M\"])\n",
        "    S, D, I, C = c[\"S\"], c[\"D\"], c[\"I\"], c[\"C\"]\n",
        "    wer = (S + D + I) / N\n",
        "    acc = max(0.0, 1.0 - wer)\n",
        "    # correctness = recall of reference words; precision = cleanliness of hypothesis\n",
        "    correctness = C / N\n",
        "    precision   = C / M\n",
        "    f1 = 0.0 if (correctness + precision) == 0 else 2 * correctness * precision / (correctness + precision)\n",
        "    return {\n",
        "        \"wer\": wer, \"acc\": acc,\n",
        "        \"correctness\": correctness,\n",
        "        \"precision\": precision,\n",
        "        \"f1\": f1,\n",
        "        \"ref_words\": c[\"N\"], \"hyp_words\": c[\"M\"],\n",
        "        \"S\": S, \"D\": D, \"I\": I, \"C\": C\n",
        "    }\n",
        "\n",
        "def score_pair(name, hyp_raw, ref_raw):\n",
        "    # RAW tokens\n",
        "    ref_r = normalize_raw(ref_raw).split()\n",
        "    hyp_r = normalize_raw(hyp_raw).split()\n",
        "    raw = metrics_from_tokens(ref_r, hyp_r)\n",
        "    # DOMAIN-NORMALIZED tokens\n",
        "    ref_n = normalize_atc(ref_raw).split()\n",
        "    hyp_n = normalize_atc(hyp_raw).split()\n",
        "    norm = metrics_from_tokens(ref_n, hyp_n)\n",
        "\n",
        "    return {\n",
        "        \"backend\": name,\n",
        "        \"wer_raw\": round(raw[\"wer\"], 4),\n",
        "        \"acc_raw\": round(raw[\"acc\"], 4),\n",
        "        \"wer_norm\": round(norm[\"wer\"], 4),\n",
        "        \"acc_norm\": round(norm[\"acc\"], 4),\n",
        "        \"correctness\": round(norm[\"correctness\"], 4),\n",
        "        \"precision\": round(norm[\"precision\"], 4),\n",
        "        \"f1\": round(norm[\"f1\"], 4),\n",
        "        \"ref_words\": norm[\"ref_words\"],\n",
        "        \"hyp_words\": norm[\"hyp_words\"],\n",
        "        \"S\": norm[\"S\"], \"D\": norm[\"D\"], \"I\": norm[\"I\"], \"C\": norm[\"C\"],\n",
        "        \"hyp_preview\": (str(hyp_raw)[:120] + \"…\") if hyp_raw and len(str(hyp_raw)) > 120 else str(hyp_raw or \"\")\n",
        "    }\n",
        "\n",
        "# Collect available hypotheses\n",
        "def _get(name):\n",
        "    return globals()[name] if name in globals() else None\n",
        "\n",
        "if \"ref_str\" not in globals():\n",
        "    raise RuntimeError(\"Please define `ref_str` (ground-truth transcript) first.\")\n",
        "\n",
        "pairs = [\n",
        "    (\"whisper\",   _get(\"whisper_text\")),\n",
        "    (\"wav2vec2\",  _get(\"wav2vec2_text\")),\n",
        "    (\"vosk\",      _get(\"vosk_text\")),\n",
        "    (\"s2t-small\", _get(\"s2t_transcript\")),\n",
        "]\n",
        "\n",
        "rows = [score_pair(name, hyp, ref_str) for name, hyp in pairs if hyp is not None]\n",
        "df = pd.DataFrame(rows).sort_values([\"wer_norm\", \"backend\"]).reset_index(drop=True)\n",
        "display(df)\n",
        "\n",
        "# Save CSV (same path as before to keep your workflow)\n",
        "csv_path = \"/content/asr_eval_summary.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "print(f\"Saved → {csv_path}\")\n",
        "\n",
        "# Charts: WER and Accuracy (normalized)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(df[\"backend\"], df[\"wer_norm\"])\n",
        "plt.title(\"ASR Comparison — Normalized WER (lower is better)\")\n",
        "plt.xlabel(\"backend\"); plt.ylabel(\"WER (normalized)\")\n",
        "plt.ylim(0, max(0.05, df[\"wer_norm\"].max()*1.15))\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(df[\"backend\"], df[\"acc_norm\"])\n",
        "plt.title(\"ASR Comparison — Normalized Accuracy (higher is better)\")\n",
        "plt.xlabel(\"backend\"); plt.ylabel(\"Accuracy (normalized)\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "# Optional: print best by acc_norm\n",
        "if not df.empty:\n",
        "    best = df.sort_values(\"acc_norm\", ascending=False).iloc[0]\n",
        "    print(f\"Best by acc_norm: {best['backend']} ({best['acc_norm']*100:.2f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
